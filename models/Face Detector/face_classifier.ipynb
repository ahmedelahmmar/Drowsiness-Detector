{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import joblib\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples_paths = []\n",
    "negative_samples_paths = []\n",
    "\n",
    "positive_samples_df = pd.read_csv(\"../../datasets/faces/faces.csv\")\n",
    "# # Remove images with more than one face\n",
    "# positive_samples_df = positive_samples_df.groupby('image_name').filter(lambda x: len(x) == 1).reset_index(drop=True)\n",
    "# # Shuffle the data frame\n",
    "# positive_samples_df = positive_samples_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "negative_samples_dirs = [os.path.join(\"../../datasets/faces/natural_images/\", folder) for folder in os.listdir(\"../../datasets/faces/natural_images/\") if folder != \"person\"]\n",
    "\n",
    "for dir in negative_samples_dirs: \n",
    "    for image in os.listdir(dir):\n",
    "        negative_samples_paths.append(os.path.join(dir, image))\n",
    "\n",
    "for image in os.listdir(\"../../datasets/faces/natural_images/person/\"):\n",
    "    positive_samples_paths.append(os.path.join(\"../../datasets/faces/natural_images/person/\", image))\n",
    "# for image in os.listdir(\"../../datasets/faces/images\"):\n",
    "#     positive_samples_paths.append(os.path.join(dir, image))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = []\n",
    "label_vector = []\n",
    "\n",
    "FEATURE_WINDOW_SIZE = (64, 64)\n",
    "\n",
    "win_size = FEATURE_WINDOW_SIZE \n",
    "block_size = (16, 16)\n",
    "block_stride = (8, 8)\n",
    "cell_size = (8, 8)\n",
    "num_bins = 9\n",
    "\n",
    "HOG = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, num_bins)\n",
    "\n",
    "def extract_features(Image):\n",
    "\n",
    "    gray_image = cv2.cvtColor(Image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    equalized_image = cv2.equalizeHist(gray_image)\n",
    "\n",
    "    resized_image = cv2.resize(equalized_image, FEATURE_WINDOW_SIZE)\n",
    "        \n",
    "    feature = HOG.compute(resized_image).flatten()\n",
    "\n",
    "    if feature.max() > 0:\n",
    "        feature /= feature.max()\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, image_properties in positive_samples_df.iterrows():\n",
    "\n",
    "    image_path = os.path.join(\"../../datasets/faces/images\", image_properties[\"image_name\"])\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    x0 = int(image_properties[\"x0\"])\n",
    "    x1 = int(image_properties[\"x1\"])\n",
    "    y0 = int(image_properties[\"y0\"])\n",
    "    y1 = int(image_properties[\"y1\"])\n",
    "\n",
    "    face = image[y0:y1 ,x0:x1]\n",
    "    feature_vector.append( extract_features(face) )\n",
    "    label_vector.append( 1 )\n",
    "\n",
    "\n",
    "for image_path in positive_samples_paths:\n",
    "\n",
    "    face = cv2.imread(image_path)\n",
    "    feature_vector.append( extract_features(face) )\n",
    "    label_vector.append( 1 )\n",
    "\n",
    "\n",
    "for image_path in negative_samples_paths:\n",
    "\n",
    "    not_face = cv2.imread(image_path)\n",
    "    feature_vector.append( extract_features(not_face) )\n",
    "    label_vector.append( 0 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train and Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(feature_vector, label_vector, test_size=0.05, random_state=42)\n",
    "\n",
    "face_classifer = LinearSVC().fit(x_train, y_train)\n",
    "\n",
    "score = face_classifer.score(x_test, y_test)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(score))\n",
    "\n",
    "joblib.dump(face_classifer, \"FaceClassifier_HoG{}{}{}({:.2f}).pkl\".format(win_size, block_size, cell_size ,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_boxes_min_max(boxes, min_x, min_y, max_x, max_y):\n",
    "  \"\"\"\n",
    "  Unifies a list of bounding boxes using minimum and maximum values.\n",
    "\n",
    "  Args:\n",
    "    boxes: A list of bounding boxes represented as (x_min, y_min, x_max, y_max) tuples.\n",
    "\n",
    "  Returns:\n",
    "    unified_box: A tuple representing the unified bounding box (x_min, y_min, x_max, y_max).\n",
    "  \"\"\"\n",
    "\n",
    "  overall_box = None\n",
    "\n",
    "  if not overall_box:\n",
    "    for box in boxes:\n",
    "      x_min, y_min, x_max, y_max = box\n",
    "      min_x = min(min_x, x_min)\n",
    "      min_y = min(min_y, y_min)\n",
    "      max_x = max(max_x, x_max)\n",
    "      max_y = max(max_y, y_max)\n",
    "    overall_box = (min_x, min_y, max_x, max_y)\n",
    "    \n",
    "    return overall_box\n",
    "\n",
    "def filter_boxes_with_skin_mask(boxes, skin_mask):\n",
    "  \"\"\"\n",
    "  Filters bounding boxes based on overlap with a skin mask.\n",
    "\n",
    "  Args:\n",
    "    boxes: A list of bounding boxes (x_min, y_min, x_max, y_max)\n",
    "    skin_mask: A binary mask (0 or 255) where 255 represents skin pixels.\n",
    "\n",
    "  Returns:\n",
    "    filtered_boxes: A list of remaining bounding boxes after filtering.\n",
    "  \"\"\"\n",
    "  filtered_boxes = []\n",
    "  for box in boxes:\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    # Calculate area of the bounding box\n",
    "    box_area = (x_max - x_min) * (y_max - y_min)\n",
    "    # Get the portion of the bounding box within the skin mask\n",
    "    intersection_mask = skin_mask[y_min:y_max, x_min:x_max]\n",
    "    intersection_area = cv2.countNonZero(intersection_mask)\n",
    "    # Calculate intersection-over-union (IoU) ratio\n",
    "    iou = intersection_area / box_area\n",
    "\n",
    "    # Define threshold for acceptable overlap with skin mask\n",
    "    threshold = 0.5\n",
    "\n",
    "    if iou >= threshold:\n",
    "      filtered_boxes.append(box)\n",
    "\n",
    "  return filtered_boxes  \n",
    "\n",
    "def get_bounding_box(mask):\n",
    "    \"\"\"Extracts the bounding box coordinates of a segmented region in a binary mask.\n",
    "\n",
    "    Args:\n",
    "        mask: A 2D binary mask (0s for background, 1s for foreground).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the top-left, bottom-right coordinates of the bounding box: (x0, y0, x1, y1).\n",
    "    \"\"\"\n",
    "\n",
    "    # Find non-zero pixels (segmented region)\n",
    "    non_zero_pixels = np.nonzero(mask)\n",
    "\n",
    "    # Extract the minimum and maximum row and column indices\n",
    "    min_row, min_col = np.min(non_zero_pixels, axis=1)\n",
    "    max_row, max_col = np.max(non_zero_pixels, axis=1)\n",
    "\n",
    "    # Return coordinates in desired format (x0, y0, x1, y1)\n",
    "    return min_col, min_row, max_col, max_row  # Note the order of coordinates\n",
    "\n",
    "def segment_skin(image):\n",
    "  \"\"\"\n",
    "  Segments skin pixels based on chrominance analysis.\n",
    "\n",
    "  Args:\n",
    "    image: A BGR image.\n",
    "\n",
    "  Returns:\n",
    "    skin_mask: A binary mask (0 or 255) where 255 represents skin pixels.\n",
    "  \"\"\"\n",
    "  # Convert to YCbCr color space\n",
    "  ycrcb = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
    "\n",
    "  # Define chrominance thresholds for skin\n",
    "  lower_thresh = (0, 133, 77)\n",
    "  upper_thresh = (255, 173, 127)\n",
    "\n",
    "  # Create binary mask based on thresholds\n",
    "  skin_mask = cv2.inRange(ycrcb, lower_thresh, upper_thresh)\n",
    "\n",
    "  # Apply morphological operations for smoothing\n",
    "  kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "  skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_OPEN, kernel, iterations=3)\n",
    "\n",
    "  return skin_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Real-Time Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_classifer = joblib.load(\"FaceClassifier_HoG(64, 64)(16, 16)(8, 8)(0.95).pkl\")\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "window_sizes = [(64, 64), (96, 96)]\n",
    "\n",
    "while camera.isOpened():\n",
    "\n",
    "    detected_faces = []\n",
    "\n",
    "    _, frame = camera.read()\n",
    "\n",
    "    skin_mask = segment_skin(frame)\n",
    "\n",
    "    x0, y0, x1, y1 = get_bounding_box(skin_mask)\n",
    "\n",
    "    for window_size in window_sizes:\n",
    "\n",
    "        y_stride = window_size[0]\n",
    "        x_stride = window_size[1]\n",
    "\n",
    "        for y in range(y0, y1, y_stride):\n",
    "        # for y in range(0, frame.shape[0] - window_size[0], y_stride):\n",
    "            \n",
    "            for x in range(x0, x1, x_stride):\n",
    "            # for x in range(0, frame.shape[1] - window_size[1], x_stride):\n",
    "                # Extract window and predict\n",
    "                window = frame[y : y+window_size[0], x : x+window_size[1]]\n",
    "\n",
    "                [prediction] = face_classifer.predict([extract_features(window)])\n",
    "\n",
    "                # Check prediction and register face\n",
    "                if prediction == 1:  # Adjust threshold based on your model\n",
    "                    detected_faces.append([x, y, x + window_size[1], y + window_size[0]])\n",
    "\n",
    "                    # cv2.rectangle(frame, (x, y), (x + window_size[1], y + window_size[0]), (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "    detected_faces = filter_boxes_with_skin_mask(detected_faces, skin_mask)\n",
    "\n",
    "    detected_faces = union_boxes_min_max(detected_faces, x0, y0, x1, y1)\n",
    "\n",
    "    cv2.rectangle(frame, (detected_faces[0], detected_faces[1]), (detected_faces[2], detected_faces[3]), (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"BGR Frame\", frame)\n",
    "    cv2.imshow('Skin Mask', skin_mask)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the OpenCV windows\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
