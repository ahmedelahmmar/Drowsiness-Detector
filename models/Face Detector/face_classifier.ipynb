{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import joblib\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples_paths = []\n",
    "negative_samples_paths = []\n",
    "\n",
    "positive_samples_df = pd.read_csv(\"../../datasets/faces/faces.csv\")\n",
    "# # Remove images with more than one face\n",
    "# positive_samples_df = positive_samples_df.groupby('image_name').filter(lambda x: len(x) == 1).reset_index(drop=True)\n",
    "# # Shuffle the data frame\n",
    "# positive_samples_df = positive_samples_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "negative_samples_dirs = [os.path.join(\"../../datasets/faces/natural_images/\", folder) for folder in os.listdir(\"../../datasets/faces/natural_images/\") if folder != \"person\"]\n",
    "\n",
    "for dir in negative_samples_dirs: \n",
    "    for image in os.listdir(dir):\n",
    "        negative_samples_paths.append(os.path.join(dir, image))\n",
    "\n",
    "for image in os.listdir(\"../../datasets/faces/natural_images/person/\"):\n",
    "    positive_samples_paths.append(os.path.join(\"../../datasets/faces/natural_images/person/\", image))\n",
    "# for image in os.listdir(\"../../datasets/faces/images\"):\n",
    "#     positive_samples_paths.append(os.path.join(dir, image))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = []\n",
    "label_vector = []\n",
    "\n",
    "FEATURE_WINDOW_SIZE = (64, 64)\n",
    "\n",
    "win_size = FEATURE_WINDOW_SIZE \n",
    "block_size = (32, 32)\n",
    "block_stride = block_size\n",
    "cell_size = (16, 16)\n",
    "num_bins = 9\n",
    "\n",
    "HOG = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, num_bins)\n",
    "\n",
    "def extract_features(Image):\n",
    "\n",
    "    resized_image = cv2.resize(Image, FEATURE_WINDOW_SIZE)\n",
    "        \n",
    "    feature = HOG.compute(resized_image).flatten()\n",
    "\n",
    "    # feature = local_binary_pattern(resized_image, lbp_n_points, lbp_radius, method='uniform').flatten()\n",
    "\n",
    "    # i_image = integral_image(resized_image)\n",
    "    # feature = haar_like_feature(i_image, 0, 0, FEATURE_WINDOW_SIZE[0], FEATURE_WINDOW_SIZE[1])\n",
    "\n",
    "    if feature.max() > 0:\n",
    "        feature /= feature.max()\n",
    "\n",
    "    return feature\n",
    "\n",
    "\n",
    "for index, image_properties in positive_samples_df.iterrows():\n",
    "\n",
    "    image_path = os.path.join(\"../../datasets/faces/images\", image_properties[\"image_name\"])\n",
    "    gray_image = cv2.imread(image_path, 0)\n",
    "\n",
    "    x0 = int(image_properties[\"x0\"])\n",
    "    x1 = int(image_properties[\"x1\"])\n",
    "    y0 = int(image_properties[\"y0\"])\n",
    "    y1 = int(image_properties[\"y1\"])\n",
    "\n",
    "    gray_face = gray_image[y0:y1 ,x0:x1]\n",
    "    feature_vector.append(extract_features(gray_face))\n",
    "    label_vector.append(1)\n",
    "\n",
    "\n",
    "for image_path in positive_samples_paths:\n",
    "\n",
    "    gray_face = cv2.imread(image_path, 0)\n",
    "    feature_vector.append(extract_features(gray_face))\n",
    "    label_vector.append(1)\n",
    "\n",
    "\n",
    "for image_path in negative_samples_paths:\n",
    "\n",
    "    gray_not_face = cv2.imread(image_path, 0)\n",
    "    feature_vector.append(extract_features(gray_not_face))\n",
    "    label_vector.append(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train and Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Tools\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Tools\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['FaceClassifier(64, 64)(0.92).pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(feature_vector, label_vector, test_size=0.05, random_state=42)\n",
    "\n",
    "face_classifer = LinearSVC().fit(x_train, y_train)\n",
    "\n",
    "score = face_classifer.score(x_test, y_test)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(score))\n",
    "\n",
    "joblib.dump(face_classifer, \"FaceClassifier{}({:.2f}).pkl\".format(FEATURE_WINDOW_SIZE ,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_skin(image):\n",
    "  \"\"\"\n",
    "  Segments skin pixels based on chrominance analysis.\n",
    "\n",
    "  Args:\n",
    "    image: A BGR image.\n",
    "\n",
    "  Returns:\n",
    "    skin_mask: A binary mask (0 or 255) where 255 represents skin pixels.\n",
    "  \"\"\"\n",
    "  # Convert to YCbCr color space\n",
    "  ycrcb = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
    "\n",
    "  # Define chrominance thresholds for skin\n",
    "  lower_thresh = (0, 133, 77)\n",
    "  upper_thresh = (255, 173, 127)\n",
    "\n",
    "  # Create binary mask based on thresholds\n",
    "  skin_mask = cv2.inRange(ycrcb, lower_thresh, upper_thresh)\n",
    "\n",
    "  # Apply morphological operations for smoothing\n",
    "  kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "  skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "  skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "  return skin_mask\n",
    "\n",
    "def union_boxes_min_max(boxes):\n",
    "  \"\"\"\n",
    "  Unifies a list of bounding boxes using minimum and maximum values.\n",
    "\n",
    "  Args:\n",
    "    boxes: A list of bounding boxes represented as (x_min, y_min, x_max, y_max) tuples.\n",
    "\n",
    "  Returns:\n",
    "    unified_box: A tuple representing the unified bounding box (x_min, y_min, x_max, y_max).\n",
    "  \"\"\"\n",
    "  min_x, min_y, max_x, max_y = 680, 680, 0, 0\n",
    "\n",
    "  for box in boxes:\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    min_x = min(min_x, x_min)\n",
    "    min_y = min(min_y, y_min)\n",
    "    max_x = max(max_x, x_max)\n",
    "    max_y = max(max_y, y_max)\n",
    "\n",
    "  unified_box = (min_x, min_y, max_x, max_y)\n",
    "  return unified_box\n",
    "\n",
    "\n",
    "\n",
    "def filter_boxes_with_skin_mask(boxes, skin_mask):\n",
    "  \"\"\"\n",
    "  Filters bounding boxes based on overlap with a skin mask.\n",
    "\n",
    "  Args:\n",
    "    boxes: A list of bounding boxes (x_min, y_min, x_max, y_max)\n",
    "    skin_mask: A binary mask (0 or 255) where 255 represents skin pixels.\n",
    "\n",
    "  Returns:\n",
    "    filtered_boxes: A list of remaining bounding boxes after filtering.\n",
    "  \"\"\"\n",
    "  filtered_boxes = []\n",
    "  for box in boxes:\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    # Calculate area of the bounding box\n",
    "    box_area = (x_max - x_min) * (y_max - y_min)\n",
    "    # Get the portion of the bounding box within the skin mask\n",
    "    intersection_mask = skin_mask[y_min:y_max, x_min:x_max]\n",
    "    intersection_area = cv2.countNonZero(intersection_mask)\n",
    "    # Calculate intersection-over-union (IoU) ratio\n",
    "    iou = intersection_area / box_area\n",
    "\n",
    "    # Define threshold for acceptable overlap with skin mask\n",
    "    threshold = 0.3\n",
    "\n",
    "    if iou >= threshold:\n",
    "      filtered_boxes.append(box)\n",
    "\n",
    "  return filtered_boxes  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Real-Time Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "window_sizes = [(128, 128)]\n",
    "\n",
    "while camera.isOpened():\n",
    "\n",
    "    detected_faces = []\n",
    "\n",
    "    _, frame = camera.read()\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    skin_mask = segment_skin(frame)\n",
    "    \n",
    "    for window_size in window_sizes:\n",
    "\n",
    "        y_stride = window_size[0] // 8\n",
    "        x_stride = window_size[1] // 8\n",
    "\n",
    "        for y in range(0, gray_frame.shape[0] - window_size[0], y_stride):\n",
    "            for x in range(0, gray_frame.shape[1] - window_size[1], x_stride):\n",
    "                # Extract window and predict\n",
    "                window = gray_frame[y : y+window_size[0], x : x+window_size[1]]\n",
    "\n",
    "                [prediction] = face_classifer.predict([extract_features(window)])\n",
    "\n",
    "                # Check prediction and register face\n",
    "                if prediction == 1:  # Adjust threshold based on your model\n",
    "                    detected_faces.append([x, y, x + window_size[1], y + window_size[0]])\n",
    "\n",
    "\n",
    "    filtered_faces = filter_boxes_with_skin_mask(detected_faces, skin_mask)\n",
    "\n",
    "    filtered_face = union_boxes_min_max(filtered_faces)\n",
    "\n",
    "    cv2.rectangle(frame, (filtered_face[0], filtered_face[1]), (filtered_face[2], filtered_face[3]), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with the detection result\n",
    "    cv2.imshow('Face Detection', frame)\n",
    "    # Press 'q' to exit the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the OpenCV windows\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
